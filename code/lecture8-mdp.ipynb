{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, height=3, width=4):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.grid = np.zeros((height, width))\n",
    "        self.terminal_states = {(0, 3): 1, (1, 3): -1}  # (state): reward\n",
    "        self.living_reward = -0.04\n",
    "        self.gamma = 1.0\n",
    "        self.p_intended = 0.8\n",
    "        self.p_perpendicular = 0.1  # For each perpendicular direction\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
    "        \n",
    "    def is_valid_state(self, state):\n",
    "        row, col = state\n",
    "        if row < 0 or row >= self.height or col < 0 or col >= self.width:\n",
    "            return False\n",
    "        if (row, col) == (1, 1):  # Wall\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_transition_probs(self, state, action):\n",
    "        if state in self.terminal_states:\n",
    "            return [(state, 1.0)]\n",
    "        \n",
    "        transitions = []\n",
    "\n",
    "        perp1 = (action[1], action[0])    # Rotate 90° clockwise\n",
    "        perp2 = (-action[1], -action[0])  # Rotate 90° counterclockwise\n",
    "        \n",
    "        for next_action, prob in [(action, self.p_intended), \n",
    "                                  (perp1, self.p_perpendicular),\n",
    "                                  (perp2, self.p_perpendicular)]:\n",
    "            next_state = (state[0] + next_action[0], state[1] + next_action[1])\n",
    "            if self.is_valid_state(next_state):\n",
    "                transitions.append((next_state, prob))\n",
    "            else:\n",
    "                transitions.append((state, prob))  # Stay in current state\n",
    "                \n",
    "        return transitions\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        if state in self.terminal_states:\n",
    "            return self.terminal_states[state]\n",
    "        return self.living_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, threshold=1e-3):\n",
    "    # Initialize values\n",
    "    V = {(i, j): 0 for i in range(grid.height) for j in range(grid.width) \n",
    "            if grid.is_valid_state((i, j))}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        # Update each state\n",
    "        for state in V:\n",
    "            if state in grid.terminal_states:\n",
    "                V_new[state] = grid.get_reward(state)\n",
    "                \n",
    "            else:\n",
    "                # Calculate max_a \\sum_{s'} P(s'|s,a) V(s')\n",
    "                max_q = float('-inf')\n",
    "\n",
    "                for action in grid.actions:\n",
    "                    q = 0\n",
    "                    for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                        q += prob * V[next_state]\n",
    "                    max_q = max(max_q, q)\n",
    "                \n",
    "                V_new[state] = grid.get_reward(state) + grid.gamma * max_q\n",
    "            biggest_change = max(biggest_change, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        iteration += 1\n",
    "        \n",
    "        # Check convergence\n",
    "        if biggest_change < threshold:\n",
    "            break\n",
    "            \n",
    "    return V, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged after 20 iterations\n",
      "\n",
      "Final values:\n",
      "   0.812    0.868    0.918    1.000 \n",
      "   0.762    XXXXX    0.660   -1.000 \n",
      "   0.705    0.655    0.611    0.387 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid = GridWorld()\n",
    "V, iterations = value_iteration(grid)\n",
    "\n",
    "print(f\"\\nConverged after {iterations} iterations\")\n",
    "print(\"\\nFinal values:\")\n",
    "for i in range(grid.height):\n",
    "    for j in range(grid.width):\n",
    "        if not grid.is_valid_state((i, j)):\n",
    "            print(\"   XXXXX \", end=\"\")\n",
    "        else:\n",
    "            print(f\" {V[(i, j)]:7.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): (0, 1),\n",
       " (0, 1): (0, 1),\n",
       " (0, 2): (0, 1),\n",
       " (0, 3): None,\n",
       " (1, 0): (-1, 0),\n",
       " (1, 2): (-1, 0),\n",
       " (1, 3): None,\n",
       " (2, 0): (-1, 0),\n",
       " (2, 1): (0, -1),\n",
       " (2, 2): (0, -1),\n",
       " (2, 3): (0, -1)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_extraction(grid, V):\n",
    "    policy = {state: None for state in V}\n",
    "    \n",
    "    for state in V:\n",
    "        if state in grid.terminal_states:\n",
    "            policy[state] = None\n",
    "        \n",
    "        else:\n",
    "            max_q = float('-inf')\n",
    "            best_action = None\n",
    "            for action in grid.actions:\n",
    "                q = 0\n",
    "                for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                    q += prob * V[next_state]\n",
    "                if q > max_q:\n",
    "                    max_q = q\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "        \n",
    "    return policy\n",
    "\n",
    "policy = policy_extraction(grid, V)\n",
    "policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(grid, policy, updates=20):\n",
    "    V = {state: 0 for state in policy}\n",
    "\n",
    "    for _ in range(updates):\n",
    "        V_new = V.copy()\n",
    "        for state in V:\n",
    "            if state in grid.terminal_states:\n",
    "                V_new[state] = grid.get_reward(state)\n",
    "\n",
    "            else:\n",
    "                action = policy[state]\n",
    "                q = 0\n",
    "                for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                    q += prob * V[next_state]\n",
    "                V_new[state] = grid.get_reward(state) + grid.gamma * q\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration(grid, threshold=1e-3):\n",
    "    # Initialize policy\n",
    "    policy = {(i, j): grid.actions[0] for i in range(grid.height) for j in range(grid.width) \n",
    "            if grid.is_valid_state((i, j))}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V = policy_evaluation(grid, policy)\n",
    "        \n",
    "        # Policy improvement\n",
    "        policy_stable = True\n",
    "        for state in V:\n",
    "            old_action = policy[state]\n",
    "            max_q = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action in grid.actions:\n",
    "                q = 0\n",
    "                for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                    q += prob * V[next_state]\n",
    "                if q > max_q:\n",
    "                    max_q = q\n",
    "                    best_action = action\n",
    "                    \n",
    "            policy[state] = best_action\n",
    "            if best_action != old_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        iteration += 1\n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "    return policy, V, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged after 3 iterations\n",
      "\n",
      "Final values:\n",
      "   0.812    0.868    0.918    1.000 \n",
      "   0.762    XXXXX    0.660   -1.000 \n",
      "   0.705    0.655    0.611    0.387 \n"
     ]
    }
   ],
   "source": [
    "policy, V, iterations = policy_iteration(grid)\n",
    "\n",
    "print(f\"\\nConverged after {iterations} iterations\")\n",
    "print(\"\\nFinal values:\")\n",
    "for i in range(grid.height):\n",
    "    for j in range(grid.width):\n",
    "        if not grid.is_valid_state((i, j)):\n",
    "            print(\"   XXXXX \", end=\"\")\n",
    "        else:\n",
    "            print(f\" {V[(i, j)]:7.3f} \", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
